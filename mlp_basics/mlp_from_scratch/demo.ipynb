{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates handwritten digit classification using:\n",
        "\n",
        "- A custom-built Multi-Layer Perceptron (MLP) implemented with NumPy\n",
        "- A standard PyTorch-based MLP\n",
        "\n",
        "Both models are trained and evaluated on the MNIST dataset. The goal is to:\n",
        "\n",
        "- Build a neural network from scratch for educational clarity\n",
        "- Compare its performance with a PyTorch equivalent\n",
        "- Visualize training metrics and conclude the strengths/limitations of both approaches\n"
      ],
      "metadata": {
        "id": "J_WagqcVXmvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from models import NeuralNetMLP\n",
        "from training import custom_train, train_torch_model\n",
        "from utils import compute_mse_and_acc, plot_training_curves\n",
        "\n",
        "SEED = 10\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X = mnist.data.values.astype(np.float32)\n",
        "y = mnist.target.values.astype(np.int64)\n",
        "\n",
        "# Scale to [-1, 1]\n",
        "X = ((X / X[0].max()) - 0.5) * 2\n",
        "\n",
        "# Train, validation, test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=10000, random_state=SEED, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=5000, random_state=SEED, stratify=y_temp)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_hidden = 50\n",
        "num_classes = 10\n",
        "num_epochs = 50\n",
        "minibatch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "custom_model = NeuralNetMLP(num_features, num_hidden, num_classes)\n",
        "custom_loss, custom_train_acc, custom_valid_acc = custom_train(custom_model, X_train, y_train, X_valid,\n",
        "                                                               y_valid,num_epochs, minibatch_size, learning_rate)\n",
        "\n"
      ],
      "metadata": {
        "id": "gKKu98_CXpCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "X_train_t = torch.tensor(X_train)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.int64)\n",
        "X_valid_t = torch.tensor(X_valid)\n",
        "y_valid_t = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "valid_dataset = torch.utils.data.TensorDataset(X_valid_t, y_valid_t)\n",
        "\n",
        "torch_model = nn.Sequential(\n",
        "    nn.Linear(num_features, num_hidden),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(num_hidden, num_classes),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(torch_model.parameters(), lr=learning_rate)\n",
        "\n",
        "torch_loss, torch_train_acc, torch_valid_acc = train_torch_model(torch_model, train_dataset, valid_dataset,\n",
        "                                                                 num_epochs, minibatch_size, loss_fn, optimizer)\n",
        "\n",
        "plot_training_curves(custom_loss, custom_train_acc, custom_valid_acc,torch_loss, torch_train_acc, torch_valid_acc)\n"
      ],
      "metadata": {
        "id": "pDdHO1KTX-TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_t = torch.tensor(X_test)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "_, custom_test_acc = compute_mse_and_acc(custom_model, X_test, y_test, custom=True, minibatch_size=minibatch_size)\n",
        "_, torch_test_acc = compute_mse_and_acc(torch_model, X_test, y_test, custom=False, minibatch_size=minibatch_size)\n",
        "\n",
        "print(f\"Custom Model Test Accuracy:  {custom_test_acc * 100:.2f}%\")\n",
        "print(f\"PyTorch Model Test Accuracy: {torch_test_acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9ZtR6vqdYPf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "- The custom-built neural network achieved **~94.88% test accuracy**, surpassing the PyTorch model's **~90.63%**.\n",
        "- Although the PyTorch model used the same architecture, the custom model outperformed it, likely due to better initialization or training dynamics.\n",
        "\n",
        "### Takeaways:\n",
        "\n",
        "- Building an MLP from scratch solidifies understanding of forward and backward propagation.\n",
        "- PyTorch, while efficient, requires thoughtful setup to match custom logic (like using sigmoid + MSE).\n",
        "- This project demonstrates confidence in both mathematical underpinnings and practical ML engineering.\n",
        "\n",
        "Next steps could include:\n",
        "- Switching to ReLU + Softmax + CrossEntropyLoss for PyTorch.\n",
        "- Implementing momentum, weight decay, or Adam optimizers.\n",
        "- Extending the custom model to include modular layers and activation functions.\n"
      ],
      "metadata": {
        "id": "UXdBAtSGYVSV"
      }
    }
  ]
}