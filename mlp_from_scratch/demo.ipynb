{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron from Scratch and with PyTorch\n",
        "\n",
        "This notebook demonstrates training and evaluating both single-layer and two-layer MLP (Multi-Layer Perceptron) architectures:\n",
        "\n",
        "- Implemented from scratch using NumPy\n",
        "- Implemented using PyTorch's `nn.Module`\n",
        "\n",
        "We will train these models on the **MNIST dataset** for handwritten digit classification.\n"
      ],
      "metadata": {
        "id": "J_WagqcVXmvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from models import NeuralNetMLP, Two_Layers_NeuralNetMLP, TorchMLP, TorchMLP2\n",
        "from training import custom_train, train_custom_2layer, train_torch_model\n",
        "from utils import compute_ce_and_acc, plot_training_curves\n"
      ],
      "metadata": {
        "id": "gKKu98_CXpCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess MNIST Data\n",
        "\n",
        "We use the `fetch_openml` API to load the MNIST dataset and perform preprocessing:\n",
        "- Normalize pixel values to range [-1, 1]\n",
        "- Split into training, validation, and test sets\n"
      ],
      "metadata": {
        "id": "fIbYb_KIUPrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 10\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X = mnist.data.values.astype(np.float32)\n",
        "y = mnist.target.values.astype(np.int64)\n",
        "\n",
        "# Normalize pixel values\n",
        "X = ((X / 255.0) - 0.5) * 2\n",
        "\n",
        "# Split data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=10000, random_state=SEED, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_temp, y_temp, test_size=5000, random_state=SEED, stratify=y_temp)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = 10\n"
      ],
      "metadata": {
        "id": "s-VaRyaJUNG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train NumPy Single-Layer MLP\n",
        "\n",
        "We first train a custom neural network implemented from scratch using NumPy, with **one hidden layer**.\n",
        "This model uses:\n",
        "\n",
        "- Sigmoid activation\n",
        "- Mean Squared Error (MSE) loss\n",
        "- Manual backpropagation\n"
      ],
      "metadata": {
        "id": "6VonjL2FUcTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "NUM_EPOCHS = 50\n",
        "NUM_HIDDEN_1 = 50\n",
        "MINIBATCH_SIZE = 100\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "print(\"Training NumPy single-layer MLP...\")\n",
        "model_custom_1layer = NeuralNetMLP(num_features, NUM_HIDDEN_1, num_classes)\n",
        "\n",
        "loss_custom_1layer, acc_train_custom_1layer, acc_valid_custom_1layer = custom_train(\n",
        "    model_custom_1layer,\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    minibatch_size=MINIBATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "wOc_6laOUeIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on Test Set (Single-Layer NumPy)\n",
        "\n",
        "After training, we evaluate the model's accuracy on the **unseen test set**.\n"
      ],
      "metadata": {
        "id": "j2J-3cmoUlzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc_test_custom_1layer = compute_ce_and_acc(\n",
        "    model_custom_1layer,\n",
        "    X_test, y_test,\n",
        "    custom=True,\n",
        "    minibatch_size=MINIBATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Custom NumPy Single-Layer MLP Test Accuracy: {acc_test_custom_1layer * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Vu9PQhJ1Un3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train NumPy Two-Layer MLP\n",
        "\n",
        "Now we train a custom neural network from scratch with **two hidden layers**, implemented using NumPy.\n",
        "\n",
        "Key differences from the single-layer version:\n",
        "- Two hidden layers with sigmoid activations\n",
        "- Cross-entropy loss with softmax output\n",
        "- Manual backpropagation extended for depth\n"
      ],
      "metadata": {
        "id": "9YYOR_beUr_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models import TwoLayerNeuralNetMLP\n",
        "\n",
        "# Hidden layer sizes\n",
        "NUM_HIDDEN_1 = 50\n",
        "NUM_HIDDEN_2 = 50\n",
        "\n",
        "print(\"Training NumPy two-layer MLP...\")\n",
        "model_custom_2layer = TwoLayerNeuralNetMLP(num_features, NUM_HIDDEN_1, NUM_HIDDEN_2, num_classes)\n",
        "\n",
        "loss_custom_2layer, acc_train_custom_2layer, acc_valid_custom_2layer = custom_train(\n",
        "    model_custom_2layer,\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    minibatch_size=MINIBATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "fEHRxPXUU4z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Set (Two-Layer NumPy)\n",
        "\n",
        "We now evaluate the custom NumPy MLP with two hidden layers on the test set.\n"
      ],
      "metadata": {
        "id": "59I2ERuWU9fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc_test_custom_2layer = compute_ce_and_acc(\n",
        "    model_custom_2layer,\n",
        "    X_test, y_test,\n",
        "    custom=True,\n",
        "    minibatch_size=MINIBATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Custom NumPy Two-Layer MLP Test Accuracy: {acc_test_custom_2layer * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "zDZVQQ4yVCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train PyTorch Single-Layer MLP\n",
        "\n",
        "Next, we'll train a PyTorch model with a single hidden layer. This will help us compare how the framework handles training versus our custom NumPy implementation.\n"
      ],
      "metadata": {
        "id": "1Gr8yyBMVGeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from training import train_torch_model\n",
        "\n",
        "# Define PyTorch single-layer model\n",
        "model_torch_single = nn.Sequential(\n",
        "    nn.Linear(num_features, NUM_HIDDEN_1),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(NUM_HIDDEN_1, num_classes)\n",
        ")\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_torch_single.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Prepare datasets\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "train_dataset_torch = data_utils.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "valid_dataset_torch = data_utils.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
        "\n",
        "print(\"Training PyTorch single-layer MLP...\")\n",
        "loss_torch_single, train_acc_torch_single, valid_acc_torch_single = train_torch_model(\n",
        "    model_torch_single,\n",
        "    train_dataset_torch,\n",
        "    valid_dataset_torch,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    minibatch_size=MINIBATCH_SIZE,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "yn1LYO-YVQPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Set (PyTorch Single-Layer)\n",
        "\n",
        "Evaluating the PyTorch single-layer model on the test data.\n"
      ],
      "metadata": {
        "id": "ON81IK_cVYpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc_test_torch_single = compute_ce_and_acc(\n",
        "    model_torch_single,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    custom=False,\n",
        "    minibatch_size=MINIBATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"PyTorch Single-Layer MLP Test Accuracy: {acc_test_torch_single * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "sZzuSFuyVVdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train PyTorch Two-Layer MLP\n",
        "\n",
        "Now, we'll train a PyTorch model with two hidden layers, allowing us to compare deeper architectures.\n"
      ],
      "metadata": {
        "id": "XgDZh4A2VpL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define PyTorch two-layer model\n",
        "model_torch_two = nn.Sequential(\n",
        "    nn.Linear(num_features, NUM_HIDDEN_1),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(NUM_HIDDEN_1, NUM_HIDDEN_2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(NUM_HIDDEN_2, num_classes)\n",
        ")\n",
        "\n",
        "# Loss and optimizer (reuse)\n",
        "optimizer_two = torch.optim.SGD(model_torch_two.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Training PyTorch two-layer MLP...\")\n",
        "loss_torch_two, train_acc_torch_two, valid_acc_torch_two = train_torch_model(\n",
        "    model_torch_two,\n",
        "    train_dataset_torch,\n",
        "    valid_dataset_torch,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    minibatch_size=MINIBATCH_SIZE,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer_two\n",
        ")\n"
      ],
      "metadata": {
        "id": "GY0If9JwVcjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Set (PyTorch Two-Layer)\n",
        "\n",
        "Evaluating the PyTorch two-layer model on the test data.\n"
      ],
      "metadata": {
        "id": "hVXvhuv5VyId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc_test_torch_two = compute_ce_and_acc(\n",
        "    model_torch_two,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    custom=False,\n",
        "    minibatch_size=MINIBATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"PyTorch Two-Layer MLP Test Accuracy: {acc_test_torch_two * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "j6XFFdPMVz2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Curves Comparison\n",
        "\n",
        "Let's visualize and compare the training loss and accuracy curves for:\n",
        "\n",
        "- Custom NumPy single-layer MLP  \n",
        "- PyTorch single-layer MLP  \n",
        "- PyTorch two-layer MLP  \n"
      ],
      "metadata": {
        "id": "JtNZ6mUSV4eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss_custom, label='Custom Single-Layer Loss')\n",
        "plt.plot(epochs, loss_torch_single, label='PyTorch Single-Layer Loss')\n",
        "plt.plot(epochs, loss_torch_two, label='PyTorch Two-Layer Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc_custom, label='Custom Single-Layer Accuracy')\n",
        "plt.plot(epochs, train_acc_torch_single, label='PyTorch Single-Layer Accuracy')\n",
        "plt.plot(epochs, train_acc_torch_two, label='PyTorch Two-Layer Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Accuracy (%)')\n",
        "plt.title('Training Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ioYzJ664V-vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we implemented and trained multilayer perceptron (MLP) models from scratch using NumPy and compared them with equivalent PyTorch implementations.\n",
        "\n",
        "Key takeaways:\n",
        "- The custom NumPy implementation helps deepen understanding of the underlying math and mechanics of neural networks.\n",
        "- PyTorch models offer more flexibility and efficiency, especially with automatic differentiation and GPU acceleration.\n",
        "- Adding more hidden layers (going from one to two layers) generally improved the model's learning capacity and accuracy.\n",
        "- Visualizing training curves enables us to better understand model convergence and compare different architectures.\n",
        "\n",
        "This hands-on approach bridges theory and practice, preparing you to build and experiment with more advanced neural networks in real-world applications.\n"
      ],
      "metadata": {
        "id": "d8KpIc8KWB2Q"
      }
    }
  ]
}